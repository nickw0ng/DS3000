{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DS3000 Lecture 7 and 8\n",
    "\n",
    "### Admin:\n",
    "- HW 3 due on Monday\n",
    "- HW 4 posted on Tuesday and due on Friday\n",
    "- Lecture 9 will be a lab. Finish the lab will earn 1 extra credict\n",
    "- Project description will be released this week\n",
    "\n",
    "### Content:\n",
    "- OpenWeather API pipeline\n",
    "- Intro to Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipeline: What is it?\n",
    "\n",
    "A data pipeline is a collection of functions* which split all the functionality of our data collection and processing\n",
    "\n",
    "(*can be other structures too, but it may be easier to first understand each as a function)\n",
    "\n",
    "\n",
    "# Why build a data pipeline?\n",
    "\n",
    "- Allows pipeline to be run in parts (rather than the whole thing)\n",
    "- Allows pipeline to be built by different programmers working on different parts in parallel\n",
    "- Allows us to test each piece of our code seperately\n",
    "- Allows for modification / re-use of different sections\n",
    "\n",
    "What we call a \"Data Pipeline\" here is a specific instance of \"Factoring\" a piece of software, splitting up its functionality into pieces.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OpenWeather API Pipeline Activity\n",
    "\n",
    "OpenWeather API offers a few different queries (see [here](https://openweathermap.org/api) for details):\n",
    "- 3-hour Forecast 5 days (which we have access to)\n",
    "- Air Pollution API\n",
    "- etc.\n",
    "\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Build a library of functions which can be pieced together to support the collection, cleaning and display of features from OpenWeather into a scatter plot of two features.\n",
    "\n",
    "### Lets design one together: \n",
    "\n",
    "(think: input/outputs -> handwritten notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plan out a pipeline\n",
    "\n",
    "Write a few 'empty' functions including little more than the docstring:\n",
    "\n",
    "```python\n",
    "def some_fnc(a_string, a_list):\n",
    "    \"\"\" processes a string and a list (somehow)\n",
    "    \n",
    "    Args:\n",
    "        a_string (str): an input string which ...\n",
    "        a_list (list): a list which describes ...\n",
    "        \n",
    "    Returns:\n",
    "        output (dict): the output dict which is ...\n",
    "    \"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "and a script which uses them:\n",
    "\n",
    "```python\n",
    "# inputs (not necessarily complete)\n",
    "lat = -42\n",
    "lon = 73\n",
    "\n",
    "some_output = some_fnc(lat, lon)\n",
    "some_other_output = some_other_fnc(some_output)\n",
    "\n",
    "```\n",
    "\n",
    "which would, if the functions worked, produce a graph like this (note: this starts Oct 6, because I made it yesterday):\n",
    "\n",
    "<img src=\"https://i.ibb.co/Ct0JtRJ/newplot-1.png\" width=500\\img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What might these empty functions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "def openweather_onecall(latlon_tuple, api_key, units='imperial'):\n",
    "    \"\"\" returns weather data from one location via onecall\n",
    "    \n",
    "    https://openweathermap.org/api/one-call-api \n",
    "    \n",
    "    Args:\n",
    "        latlon_tuple (tuple): first element is lattitude,\n",
    "            second is longitude\n",
    "        api_key (str): API key required to access data\n",
    "        units (str): 'imperial', 'standard', 'metric'\n",
    "        \n",
    "    Returns:\n",
    "        weather_dict (dict): a nested dictionary (tree) which\n",
    "            contains weather data\n",
    "    \"\"\"\n",
    "    # build url\n",
    "    lat, lon = latlon_tuple\n",
    "    url = f'https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&APPID={api_key}&units={units}'\n",
    "\n",
    "    # get url as a string\n",
    "    url_text = requests.get(url).text\n",
    "    \n",
    "    # convert json to a nested dict\n",
    "    weather_dict = json.loads(url_text)\n",
    "    \n",
    "    return weather_dict\n",
    "\n",
    "def get_clean_df_daily(weather_dict):\n",
    "    \"\"\" formats daily_dict to a pandas data frame\n",
    "    \n",
    "    see https://openweathermap.org/api/one-call-api for\n",
    "    full daily_dict specification\n",
    "    \n",
    "    Args:\n",
    "        weather_dict (list): list of dictionaries of 3-hour window\n",
    "            weather features\n",
    "            \n",
    "    Returns:\n",
    "        df_daily (pd.DataFrame): each row is weather from 3-hour window\n",
    "    \"\"\"\n",
    "    hour_dict = weather_dict['list'][0]['main']\n",
    "    hour_dict['datetime'] = weather_dict['list'][0]['dt_txt']\n",
    "\n",
    "    df_hourly = pd.Series(hour_dict)\n",
    "\n",
    "    df_hourly = pd.DataFrame(df_hourly).transpose()\n",
    "    \n",
    "    index = 0\n",
    "    for hour_index in weather_dict['list']:\n",
    "\n",
    "        hour_dict = hour_index['main']\n",
    "        hour_dict['datetime'] = hour_index['dt_txt']\n",
    "\n",
    "        s_hour = pd.Series(hour_dict)\n",
    "    \n",
    "        #df_hourly = df_hourly.append(s_hour, ignore_index=True)\n",
    "        df_hourly.loc[str(index),:] = s_hour\n",
    "    \n",
    "        index = index + 1\n",
    "\n",
    "    df_hourly = df_hourly.iloc[1:,]   \n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "def scatter_plotly(df, feat_x, feat_y, f_html='scatter.html'):\n",
    "    \"\"\" creates a plotly scatter plot, exports as html \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): pandas dataframe\n",
    "        x_feat (str): x axis of scatter\n",
    "        y_feat (str): y axis of scatter\n",
    "        f_html (str): output html file\n",
    "        \n",
    "    Returns:\n",
    "        f_html (str): output html file\n",
    "    \"\"\"\n",
    "    # creat scatter plot\n",
    "    fig = px.scatter(df, x=feat_x, y=feat_y)\n",
    "\n",
    "    # export scatter to html\n",
    "    plotly.offline.plot(fig, filename=f_html)\n",
    "    \n",
    "    return f_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "feat_x = 'datetime'\n",
    "feat_y = 'temp_max'\n",
    "latlon_tuple = -42, 70\n",
    "units = 'imperial'\n",
    "api_key = '2afdede234eabfa52612efba55bcc8ac'\n",
    "\n",
    "# get data\n",
    "weather_dict = openweather_onecall(latlon_tuple, \n",
    "                                   units=units,\n",
    "                                   api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sea_level</th>\n",
       "      <th>grnd_level</th>\n",
       "      <th>humidity</th>\n",
       "      <th>temp_kf</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.66</td>\n",
       "      <td>34.23</td>\n",
       "      <td>43.66</td>\n",
       "      <td>43.66</td>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-07-13 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.72</td>\n",
       "      <td>34.72</td>\n",
       "      <td>43.72</td>\n",
       "      <td>43.81</td>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "      <td>63</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2024-07-13 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>43.43</td>\n",
       "      <td>43.5</td>\n",
       "      <td>1019</td>\n",
       "      <td>1019</td>\n",
       "      <td>1019</td>\n",
       "      <td>62</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2024-07-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.43</td>\n",
       "      <td>34.02</td>\n",
       "      <td>43.43</td>\n",
       "      <td>43.43</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-07-14 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.56</td>\n",
       "      <td>34.18</td>\n",
       "      <td>43.56</td>\n",
       "      <td>43.56</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-07-14 06:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp feels_like temp_min temp_max pressure sea_level grnd_level humidity  \\\n",
       "0  43.66      34.23    43.66    43.66     1018      1018       1018       64   \n",
       "1  43.72      34.72    43.72    43.81     1018      1018       1018       63   \n",
       "2   43.5       34.2    43.43     43.5     1019      1019       1019       62   \n",
       "3  43.43      34.02    43.43    43.43     1020      1020       1020       57   \n",
       "4  43.56      34.18    43.56    43.56     1022      1022       1022       56   \n",
       "\n",
       "  temp_kf             datetime  \n",
       "0       0  2024-07-13 18:00:00  \n",
       "1   -0.05  2024-07-13 21:00:00  \n",
       "2    0.04  2024-07-14 00:00:00  \n",
       "3       0  2024-07-14 03:00:00  \n",
       "4       0  2024-07-14 06:00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean weather dict (make dataframe from dict, process timestamps etc)\n",
    "df_daily = get_clean_df_daily(weather_dict)\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make scatter\n",
    "f_html = scatter_plotly(df_daily, feat_x=feat_x, feat_y=feat_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "* Using programs or scripts to pretend to browse websites, examine the content on those websites, retrieve and extract data from those websites\n",
    "* Why scrape?\n",
    "    * if an API is available for a service, we will nearly always prefer the API to scraping\n",
    "    * ... but not all services have APIs or the available APIs are too expensive for our project\n",
    "    * newly published information might not yet be available through ready datasets\n",
    "* Downsides of scraping:\n",
    "    * no reference documentation (unlike APIs)\n",
    "    * no guarantee that a webpage we scrape will look and work the same way the next day (might need to rewrite the whole scraper)\n",
    "    * if it violates the terms of service it might be seen as a felony (https://www.aclu.org/cases/sandvig-v-barr-challenge-cfaa-prohibition-uncovering-racial-discrimination-online)\n",
    "    * legal and moral greyzone (even if the ToS does not disallow it, somebody has to pay for the traffic and when you're scraping you're not looking at ads)\n",
    "    * ... but everbody does it anyway (https://www.hollywoodreporter.com/thr-esq/genius-says-it-caught-google-lyricfind-redhanded-stealing-lyrics-400m-suit-1259383)\n",
    "* Web scraping pipeline:\n",
    "    * because the webpages might change their structure it's extra important to keep the crawling/extraction step separate from transformations and loading\n",
    "    * ETL (Extraction-Transform-Load):\n",
    "        * **Crawl**: open a given URL using requests and get the HTML source;\n",
    "        * **Extract**: extract interesting content from the webpage's source.\n",
    "        * **Transform**: our usual unit conversions, etc\n",
    "        * **Load**: representing the data in an easy way for storage and analysis\n",
    "    * **Pro tip**: it's usually a good idea to not only store the transformed data, but also the raw HTML source - because the webpages might change and we might be late to realize we're not extracting right. If we have the original HTML source we can go back to it\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, whether is it OK to be scrapped?\n",
    "\n",
    "robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best case scenario\n",
    "Some webpages publish their data in the form of simple tables. In these (rare) cases we can just use pandas .read_html to scrape this data:\n",
    "\n",
    "https://www.espn.com/nba/team/stats/_/name/bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseball instead of basketball?\n",
    "# https://www.baseball-reference.com/teams/BOS/2022.shtml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messy Data\n",
    "\n",
    "Notice that the baseball data are quite a bit messier than the basketball data. In web scraping, you are beholden to the format of the website (.html) and will almost certainly have to clean data (sometimes extensively) after scraping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic HTML\n",
    "Web pages are written in HTML. The source of https://sapiezynski.com/ds3000/scraping/01.html looks like this:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "        <!-- comments in HTML are marked like this -->\n",
    "        \n",
    "        <!-- the head tag contains the meta information not displayed but helps browsers render the page -->\n",
    "    </head>\n",
    "    <body>\n",
    "         <!-- This is the body of the document that contains all the visible elements.-->\n",
    "        <h1>Heading 1</h1>\n",
    "        <h2>This is what heading 2 looks like</h2>\n",
    "        \n",
    "        <p>Text is usually in paragraphs.\n",
    "            New lines and multiple consecutive whitespace characters are ignored.</p>\n",
    "\n",
    "<p>Unlike in python indentation is only a good practice but it doesn't change functionality. In fact, all of this HTML could be (and often is in real webpages) just writen as a single line.</p>   \n",
    "        \n",
    "        <p>Links are created using the \"a\" tag: \n",
    "            <a href=\"https://www.google.com\">Click here to google.</a>\n",
    "            href is an attirbute of the a tag that specify where the link points to.</p>\n",
    "        \n",
    "        \n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "The keywords in `<>` brackets are called tags. They open with `<tag>` and close with `</tag>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the html content in Python\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>nytimes.com</title><style>#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}</style></head><body style=\"margin:0\"><p id=\"cmsg\">Please enable JS and disable any ad blocker</p><script data-cfasync=\"false\">var dd={'rt':'c','cid':'AHrlqAAAAAMAg2FSlyqAvKkASEY_Dw==','hsh':'499AE34129FA4E4FABC31582C3075D','t':'bv','s':17439,'e':'f5bdd29d2b571013e8a759916935edc6c24068c138a78c1ab611e1d898c28a91','host':'geo.captcha-delivery.com'}</script><script data-cfasync=\"false\" src=\"https://ct.captcha-delivery.com/c.js\"></script></body></html>\n"
     ]
    }
   ],
   "source": [
    "# sometimes this doesn't quite work the way you want (c'est la vie with web scraping)\n",
    "response2 = requests.get('https://www.nytimes.com/2019/03/10/style/what-is-tik-tok.html')\n",
    "print(response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup\n",
    "\n",
    "Even if the .html does look relatively clean, it's still just a big string. How can we deal with it? Luckily there is a module made for just this purpose, and it's even a magic command which we can install directly in jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yangx\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yangx\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=c09542241be07caaa3a7ba8e73bbed263656c48a06e7a2d08745e378c42e81b5\n",
      "  Stored in directory: c:\\users\\yangx\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\yangx\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://sapiezynski.com/ds3000/scraping/01.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `.find_all()` on subtrees of soup object\n",
    "\n",
    "\n",
    "The `.find_all()` method works not only on the whole `soup` object, but also on subtrees of the soup object.  \n",
    "\n",
    "Consider the site at https://sapiezynski.com/ds3000/scraping/02.html:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <body>\n",
    "        <p>The links in this paragraph point to search engines, like <a href=\"https://duckduckgo.com\">DuckDuckGo</a>, <a href=\"https://google.com\">Google</a>, <a href=\"https://bing.com\">Bing</a></p>\n",
    "        \n",
    "        <p>The links in this paragraph point to Internet browsers, like <a href=\"https://firefox.com\">Firefox</a>, <a href=\"https://chrome.com\">Chrome</a>, <a href=\"https://opera.com\">Opera</a></p>.\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "**Goal**: Grab links from the first paragraph only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://sapiezynski.com/ds3000/scraping/02.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some syntactic sugar: \n",
    "To get the first tag under a soup object, refer to it as an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying if tags exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test if a tag exists in a soup object by looking for the first instance of this tag and comparing it to `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tags by `class_`\n",
    "\n",
    "**Tip**: This is often one of the most useful ways to localize a particular part of a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soup\n",
    "url = 'https://www.allrecipes.com/search?q=cheese+fondue'\n",
    "responses = requests.get(url)\n",
    "soup = BeautifulSoup(responses.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **goal** is to get a list of recipes.  Maybe we should find all the `div` tags? What about `span` tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags can have multiple \"classes\" they belong to.  For example, in https://www.allrecipes.com/search?q=cheese+fondue the first recipe is encapsulated in this html tag:\n",
    "\n",
    "    <span class=\"card__title\"><span class=\"card__title-text\">Cheese Fondue</span></span>\n",
    "    \n",
    "So this particular span tag belongs to classes:\n",
    "- `card__title`\n",
    "- `card__title-text`\n",
    "    \n",
    "I suspect only our target recipes belong to the `card__title-text` class.  Lets find them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tags by `id`\n",
    "\n",
    "Nearly the same as finding by class, but you'll look for `id=` in the html and pass it to the `id` keyword of `soup.find_all()`.\n",
    "\n",
    "**Goal**: Get the footer from: https://www.scrapethissite.com/\n",
    "\n",
    "\n",
    "\n",
    "```html\n",
    "<section id=\"footer\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"row\">\n",
    "                <div class=\"col-md-12 text-center text-muted\">\n",
    "                    Lessons and Videos © Hartley Brody 2018\n",
    "                </div><!--.col-->\n",
    "            </div><!--.row-->\n",
    "        </div><!--.container-->\n",
    "    </section>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soup from url\n",
    "url = 'https://www.scrapethissite.com/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can combine all searches shown above:\n",
    "- tag\n",
    "    - p (paragraph)\n",
    "    - a (link)\n",
    "    - div, span, ...\n",
    "- tag class\n",
    "- tag id\n",
    "\n",
    "```python\n",
    "# finds all links (tag type = 'a'), with given class and id\n",
    "soup.find_all('a', class_='fancy-link', id='blue')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: \n",
    "\n",
    "**Goal:** Get a list of recipe names from www.allrecipes.com like we did for:\n",
    "\n",
    "https://www.allrecipes.com/search?q=cheese+fondue\n",
    "\n",
    "1. Write function `crawl_recipes(query)` which:\n",
    "    * takes the search phrase (the ingredient) as input argument\n",
    "    * builds the correct url that leads directly to the page that lists the recipes\n",
    "    * uses `requests` to get the content of this page returns the html text of the page\n",
    "1. Write `extract_recipes(text)` which:\n",
    "    * takes the text returned by `crawl_recipes` as argument\n",
    "    * builds a BeautifulSoup object out of that text \n",
    "    * finds names of all recipes\n",
    "        - to identify which tags / classes to `find_all()`, open the page in your browser and \"inspect\" \n",
    "        - start from the recipe object above, and call another `find_all()` to zoom into the recipe name itself\n",
    "    * returns the list of recipe names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new function that will help if you wish to query multiple words:\n",
    "\n",
    "`string.replace()`\n",
    "\n",
    "So, if you wish to turn `cheese fondue` into `cheese+fondue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_recipes(query):\n",
    "    \"\"\" gets html of from allrecipes.com to search query\n",
    "    \n",
    "    Args:\n",
    "        query (str): search string\n",
    "        \n",
    "    Returns:\n",
    "        html_str (str): html response from allreceipes.com\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def extract_recipes(text):\n",
    "    \"\"\" builds list of recipe names from allrecipies html\n",
    "    \n",
    "    Args:\n",
    "        html_str (str): html response from allrecipes.com, see crawl_recipes()\n",
    "        \n",
    "    Returns:\n",
    "        recipe_list (list): list of recipes\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting info from each recipe's own page:\n",
    "\n",
    "When we interact with the webpage in the browser, clicking on the header with the recipe name leads us to the actual recipe. Let's have a look at how it's done. Here is the link (`<a >` tag) for the first and third cards of the meatloaf search:\n",
    "\n",
    "```html\n",
    "<a class=\"comp mntl-card-list-items mntl-document-card mntl-card card card--no-image\" \n",
    "   data-cta=\"\" \n",
    "   data-doc-id=\"6663943\" \n",
    "   data-ordinal=\"1\" \n",
    "   data-tax-levels=\"\" \n",
    "   href=\"https://www.allrecipes.com/recipe/219171/classic-meatloaf/\" \n",
    "   id=\"mntl-card-list-items_1-0\">\n",
    "```\n",
    "\n",
    "```html\n",
    "<a class=\"comp mntl-card-list-items mntl-document-card mntl-card card card--no-image\" \n",
    "   data-cta=\"\" \n",
    "   data-doc-id=\"6663443\" \n",
    "   data-ordinal=\"3\" \n",
    "   data-tax-levels=\"\" \n",
    "   href=\"https://www.allrecipes.com/recipe/223381/melt-in-your-mouth-meat-loaf/\" \n",
    "   id=\"mntl-card-list-items_1-0-2\">\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding `href` to our dataframe of recipes\n",
    "\n",
    "Let's modify our `extract_recipes()` function such that rather than returning just the names of the dishes, it returns a list of dictionaries, where each dictionary has the `name` and `url` fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `from_dict`\n",
    "\n",
    "First, a useful tool to turn a dictionary into a data frame where the keys are features (columns) and the values are lists that correspond to the values of the features (rows) is the `pd.DataFrame.from_dict()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>who</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>why</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2   col3\n",
       "0     1     6    who\n",
       "1     2     7   what\n",
       "2     3     8   when\n",
       "3     4     9  where\n",
       "4     5    10    why"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_dict = {'col1': [1,2,3,4,5],\n",
    "                'col2': [6,7,8,9,10],\n",
    "                'col3': ['who', 'what', 'when', 'where', 'why']}\n",
    "pd.DataFrame.from_dict(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recipes(text):\n",
    "    \"\"\" builds list of recipe names from allrecipies html\n",
    "    \n",
    "    Args:\n",
    "        html_str (str): html response from allrecipes.com, see crawl_recipes()\n",
    "        \n",
    "    Returns:\n",
    "        df_recipe (pd.DataFrame): dataframe of recipes\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulations\n",
    "- `.split()` & `.join()`\n",
    "- `.strip()`\n",
    "- `.replace()`\n",
    "- `.upper()` & `.lower()`\n",
    "\n",
    "Visting [a specific recipe's page](https://www.allrecipes.com/recipe/219171/classic-meatloaf/) yields data stored in a string.  The methods above allow us to extract this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit specific recipe's page\n",
    "url = 'https://www.allrecipes.com/recipe/283561/classic-cheese-fondue/'\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Write two functions: `extract_prep_info()` and `extract_nutrition()`, which both accept a url of a particular recipe (see examples above) and return dictionaries of the prep in of nutritional information, respectively. For example:\n",
    "\n",
    "```python\n",
    "url = 'https://www.allrecipes.com/recipe/283561/classic-cheese-fondue/'\n",
    "extract_prep_info(url)\n",
    "extract_nutrition(url)\n",
    "\n",
    "```\n",
    "\n",
    "yields:\n",
    "\n",
    "```python\n",
    "prep_info_dict = {'Prep Time': '10 mins',\n",
    "                  'Cook Time': '15 mins',\n",
    "                  'Total Time': '25 mins',\n",
    "                  'Servings': '10',\n",
    "                  'Yield': '10 servings'}\n",
    "\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "nutr_info_dict = {'Total Fat': '14g',\n",
    "                  'Saturated Fat': '9g',\n",
    "                  'Cholesterol': '46mg',\n",
    "                  'Sodium': '179mg',\n",
    "                  'Total Carbohydrate': '3g',\n",
    "                  'Total Sugars': '1g',\n",
    "                  'Protein': '13g',\n",
    "                  'Vitamin C': '0mg',\n",
    "                  'Calcium': '461mg',\n",
    "                  'Iron': '0mg',\n",
    "                  'Potassium': '67mg'}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prep_info(url):\n",
    "    \"\"\" returns a dictionary of recipe preparation info \n",
    "    \n",
    "    Args:\n",
    "        url (str): location of all recipes \"recipe\"\n",
    "        \n",
    "    Returns:\n",
    "        prep_info_dict (dict): keys are features ('prep'), \n",
    "            vals are str that describe feature ('20 mins')\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nutrition(url):\n",
    "    \"\"\" returns a dictionary of nutrition info \n",
    "    \n",
    "    Args:\n",
    "        url (str): location of all recipes \"recipe\"\n",
    "        \n",
    "    Returns:\n",
    "        nutr_dict (dict): keys are molecule types ('fat'), \n",
    "            vals are str of quantity ('24 g')\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.allrecipes.com/recipe/283561/classic-cheese-fondue/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing numeric values (float/int) from messy strings\n",
    "\n",
    "- We have strings which describe recipe nutrition info (`'100 mg'`)\n",
    "- We want numeric data types (`float, int`) so that we can plot and operate on these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest of Class (Go slowly; if we don't finish we can next week)\n",
    "Complete the `extract_nutrition()` below such that:\n",
    "\n",
    "```python\n",
    "# get / extract a data frame of recipes (only name and href)\n",
    "str_query = 'boston cream pie'\n",
    "html_str = crawl_recipes(str_query)\n",
    "df_recipe = extract_recipes(html_str)\n",
    "\n",
    "for row_idx in range(df_recipe.shape[0]):\n",
    "    # get / extract nutrition info for a particular recipe\n",
    "    recipe_url = df_recipe.loc[row_idx, 'href']\n",
    "    nutr_dict = extract_nutrition(recipe_url)\n",
    "    \n",
    "    # add each new nutrition feature to the dataframe\n",
    "    # only if there ARE nutrition features\n",
    "    if len(nutr_dict) != 0:\n",
    "        for nutr_feat, nutr_val in nutr_dict.items():\n",
    "            df_recipe.loc[row_idx, nutr_feat] = nutr_val\n",
    "    else:\n",
    "        df_recipe = df_recipe.drop(row_idx, axis=0)\n",
    "\n",
    "```\n",
    "\n",
    "generates the `df_recipe`:\n",
    "\n",
    "|    | name                           | href                                              | Total Fat | Saturated Fat | Cholesterol | Sodium | Total Carbohydrate | Dietary Fiber | Total Sugars | Protein | Vitamin C | Calcium | Iron | Potassium |\n",
    "|----|--------------------------------|---------------------------------------------------|-----------|---------------|-------------|--------|--------------------|---------------|--------------|---------|-----------|---------|------|-----------|\n",
    "| 0  | Chef John's Boston Cream Pie   | https://www.allrecipes.com/recipe/220942/chef-... | 41        | 17            | 199         | 514    | 72                 | 2             | 46           | 10      | 0         | 168     | 2    | 230       |\n",
    "| 1  | Boston Cream Pie               | https://www.allrecipes.com/recipe/8138/boston-... | 13        | 6             | 61          | 230    | 47                 | 1             | 34           | 5       | 0         | 101     | 2    | 134       |\n",
    "| 2  | Boston Cream Pie I             | https://www.allrecipes.com/recipe/8137/boston-... | 15        | 9             | 94          | 223    | 43                 | 1             | 26           | 5       | 0         | 97      | 2    | 95        |\n",
    "| 3  | Semi-Homemade Boston Cream Pie | https://www.allrecipes.com/recipe/278930/semi-... | 41        | 16            | 219         | 568    | 79                 | 3             | 53           | 11      | 0         | 186     | 3    | 194       |\n",
    "| 9  | Hot Milk Sponge Cake II        | https://www.allrecipes.com/recipe/8159/hot-mil... | 3         | 2             | 52          | 231    | 34                 | 0             | 20           | 4       | NaN       | 61      | 2    | 60        |\n",
    "| 17 | Boston Cream Dessert Cups      | https://www.allrecipes.com/recipe/213446/bosto... | 15        | 7             | 44          | 237    | 32                 | 0             | 22           | 3       | 0         | 41      | 1    | 101       |\n",
    "| 19 | Boston Creme Mini-Cupcakes     | https://www.allrecipes.com/recipe/220809/bosto... | 12        | 4             | 32          | 253    | 34                 | 0             | 24           | 3       | 0         | 62      | 1    | 100       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nutrition(url):\n",
    "    \"\"\" returns a dictionary of nutrition info \n",
    "    \n",
    "    Args:\n",
    "        url (str): location of all recipes \"recipe\"\n",
    "        \n",
    "    Returns:\n",
    "        nutr_dict (dict): keys are molecule types ('fat'), \n",
    "            vals are floats of quantity ('24 g' = 24)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.allrecipes.com/recipe/220942/chef-johns-boston-cream-pie/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nutrition(url):\n",
    "    \"\"\" returns a dictionary of nutrition info \n",
    "    \n",
    "    Args:\n",
    "        url (str): location of all recipes \"recipe\"\n",
    "        \n",
    "    Returns:\n",
    "        nutr_dict (dict): keys are molecule types ('fat'), \n",
    "            vals are floats of quantity ('24 g' = 24)\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "- get list of dictionaries corresponding to recipes (done!)\n",
    "- get dictionary of nutrition info per recipe (done!)\n",
    "- aggregating info into dataframe (see below)\n",
    "- scatter plot (up next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_recipe(str_query, recipe_limit=None):\n",
    "    \"\"\" searches for recipes and returns list, with nutrition info\n",
    "    \n",
    "    Args:\n",
    "        str_query (str): search string\n",
    "        recipe_limit (int): if passed, limits recipe (helpful\n",
    "            to speed up nutrition scraping for teaching!)\n",
    "        \n",
    "    Returns:\n",
    "        df_recipe (pd.DataFrame): dataframe, each row is recipe.\n",
    "            includes columns href, name, and nutrition facts\n",
    "    \"\"\"    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
