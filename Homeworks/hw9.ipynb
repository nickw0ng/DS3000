{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2742929d",
   "metadata": {},
   "source": [
    "# DS 3000 HW 9 $\\color{red}{\\text{SOLUTIONS}}$\n",
    "\n",
    "Due: Sunday Aug 4th @ 11:59 PM EST\n",
    "\n",
    "### Submission Instructions\n",
    "Submit this `ipynb` file to Gradescope (this can also be done via the assignment on Canvas).  To ensure that your submitted files represent your latest code, make sure to give a fresh `Kernel > Restart & Run All` just before uploading the files to gradescope.\n",
    "\n",
    "### Tips for success\n",
    "- Start early\n",
    "- Make use of Piazza\n",
    "- Make use of Office hour\n",
    "- Remember to use cells and headings to make the notebook easy to read (if a grader cannot find the answer to a problem, you will receive no points for it)\n",
    "- Under no circumstances may one student view or share their ungraded homework or quiz with another student [(see also)](http://www.northeastern.edu/osccr/academic-integrity), though you are welcome to **talk about** (not show each other) the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dcc3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below are all the modules you will need on this homework\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d38c3",
   "metadata": {},
   "source": [
    "## Part 1: Probability and odds (20 points)\n",
    "\n",
    "One of the data we have seen in this course is `ICUAdmissions.csv` file. Patients admitted to intensive care units (ICUs) are very ill, either from a serious medical event (e.g. respiratory failure from asthma) or from trauma (e.g, traffic accident). Can patient features available at admission be used to estimate the probability of survival to hospital discharge?\n",
    "\t\t\n",
    "- The dataset contains information about patient characteristics at admission, such as heart rate, diagnosis, and kidney function. Also, there are treatment information, like whehther CPR is used on the patient. \n",
    "- The variable `Status` is a factor variable with labels `Died` and `Lived`, corresponding to the levels `1` for death before discharge and `0` survival to discharge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30faa4c9-dc02-42a4-8a95-95b750a093d4",
   "metadata": {},
   "source": [
    "Here is a summary for the data when we compare whether they survived from the ICU and whether there is CPR being used on the patient: \n",
    "\n",
    "|       | No  | Yes | Sum |\n",
    "|-------|-----|-----|-----|\n",
    "| Died  | 33  | 7   | 40  |\n",
    "| Lived | 154 | 6   | 160 |\n",
    "| Sum   | 187 | 13  | 200 |\n",
    "\n",
    "Calculate following values (keep 3 decimals): \n",
    "\n",
    "- The probability that a patient died\n",
    "- The probability that a patient died without CPR\n",
    "- The probability that a patient died with CPR\n",
    "- The odds that a patient died without CPR\n",
    "- The odds that a patient died with CPR\n",
    "\n",
    "\n",
    "In the end, also use your own words to explain how probability and odds are related. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daed50f-086c-4df9-9d40-583e05b51178",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db9e908-8827-4abb-8443-d247aece5919",
   "metadata": {},
   "source": [
    "## Part 2: Fit a logistics regression (40 points)\n",
    "\n",
    "Now let's look at another medical data which logistics regression may be useful. This dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The features in the data includes radius, texture, perimeter, area, smoothness, compactness, concavity, etc, and the target is the diagnosis (benign or malignant).\n",
    "\n",
    "Fit a logistics regression to predict the `target`. Use all the other features as the X. Implement with a 5-fold cross validation and show the confusion matrix plot. You can use 0.5 as the boundary to decide whether the cancer is benign(1) or malignant(0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e95c7c-a814-49a2-a50f-921676860f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca25c72-2e53-4365-99e6-8455adf4ac5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e593e803-ba39-4ae8-8b00-42767ddd5dfe",
   "metadata": {},
   "source": [
    "## Part 3: Combine logistics regression and polynomial features (40 points)\n",
    "\n",
    "In this data, a lot of the features does have a direct relationship with the target value. Let's only look at `mean texture` and `mean compactness`. \n",
    "\n",
    "- Take out these two features, and generate a polynomial matrix with degree 2. Print the first row of the data. How many values you have?\n",
    "- Read the [function description](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) There is an attribute called get_feature_names_out(). Run it and get the names for the polynomial matrix\n",
    "- Generate another polynomial matrix with degree = 3. Include the names of the polynomial terms to the polynomial matrix (You may need to convert it back to a pandas data frame) and then fit the full data to it. Show the summary table for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd8b7a-c421-478b-85b8-7b521ae15c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
